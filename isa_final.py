# -*- coding: utf-8 -*-
"""ISA Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wSlgY-TJMuIvSIizLetuJLFwCh2lOEKQ
"""

#imports for data processing and also for model making and running
import pandas as pd
import numpy as np


import cv2
from imutils import url_to_image
from urllib.error import HTTPError

from keras.utils import get_file
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import optimizers
from keras.models import Sequential
from tensorflow.keras.applications import VGG19 as v
from keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras import regularizers

from sklearn.preprocessing import LabelEncoder

#imports for loop updates
from IPython.display import clear_output
import timeit

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('./image-Sentiment-polarity-DFE.csv')

df

data = df[['which_of_these_sentiment_scores_does_the_above_image_fit_into_best','imageurl']]

data.rename(columns = {'which_of_these_sentiment_scores_does_the_above_image_fit_into_best':'sentiment'}, inplace = True)

data

data.sort_values(by='sentiment',ascending=False,inplace=True,ignore_index=True)

data

x = data.iloc[:1000,:]

y = data.iloc[13000:14000,:]

truncated_data = x

truncated_data = truncated_data.append(y,ignore_index=True)

truncated_data

truncated_data = truncated_data.sample(frac=1,ignore_index=True)

truncated_data

truncated_data['sentiment'].value_counts()

training_images = truncated_data.iloc[:1000,:]
test_images = truncated_data.iloc[1000:,:]

print("Shape of new dataframes - {} , {}".format(training_images.shape, test_images.shape))

training_images['sentiment'].value_counts()

test_images['sentiment'].value_counts()

start = timeit.default_timer()

def get_data(d):
  x = []
  for i in range(len(d)):
    clear_output(wait=True)
    try:
      img = url_to_image(d[i])
    except HTTPError as err:
      if err.code == 404 or err.code == 410 or err.code == 104:
        training_images.drop([i], inplace=True)
        continue
      else:
       raise
    image = cv2.resize(img,(224,224),interpolation = cv2.INTER_AREA)
    x.append(image)

    stop = timeit.default_timer()

    if(i/len(d)*100)<5:
      expected_time = "Calculating..."
    else:
      time_perc =  timeit.default_timer()
      expected_time = np.round(((time_perc-start)/(i/len(d)))/60,2)

      print("Current Progress: ",np.round(i/len(training_images) * 100,2),"%")
      print("Current Run Time: ",np.round((stop-start) / 60,2),"minutes")
      print("Expected Run Time: ",expected_time,"minutes")
  x = np.array(x)
  return x

training_data = get_data(training_images['imageurl'])

from sklearn.preprocessing import LabelEncoder
def encode(t):
  encoder = LabelEncoder()
  
  encoder_df = pd.DataFrame(encoder.fit_transform(t[['sentiment']]))

  t = t.join(encoder_df)

  return t

training_images.reset_index(inplace=True)

training_images = encode(training_images)

training_images = training_images.dropna()

training_images

training_images.drop('index',axis=1,inplace=True)

training_images

training_label = training_images.iloc[:,2:3]

training_label

training_label_categorical = tf.keras.utils.to_categorical(training_label,num_classes=2)
training_label_categorical

training_label_categorical.shape

training_data.shape

base_model_1 = v(input_shape=(224,224,3), weights='imagenet', include_top=False)
base_model_1.trainable = False #freeze the layers
base_model_1.summary()

model = tf.keras.Sequential([
    base_model_1,
    GlobalAveragePooling2D(),
    Dropout(0.5),
    Dense(512, activation='relu',kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),
    bias_regularizer=regularizers.L2(1e-4),
    activity_regularizer=regularizers.L2(1e-5)),
    Dense(2, activation='softmax')
])
model.summary()

model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), metrics = ['acc'])

training_data = training_data*255

training_data

history = model.fit(training_data,training_label_categorical, batch_size=5, epochs=64, validation_split=0.25)

training_images.shape

model.evaluate(training_data,training_label_categorical)

test_images = truncated_data.iloc[1000:,:]
test_images.reset_index(inplace=True)
test_images

test_images.drop('index',axis=1,inplace=True)

test_images

start = timeit.default_timer()


test_data = []
d = test_images['imageurl']
for i in range(len(d)):
  clear_output(wait=True)

  try:
    img = url_to_image(d[i])
  except HTTPError  as err:
    if err.code == 404 or err.code == 410 or err.code == 104:
      d.drop([i], inplace=True)
    else:
      raise
  image = cv2.resize(img,(224,224) ,interpolation = cv2.INTER_AREA)
  test_data.append(image)

  stop = timeit.default_timer()

  if(i/len(test_images)*100)<5:
    expected_time = "Calculating..."
  else:
    time_perc =  timeit.default_timer()
    expected_time = np.round(((time_perc-start)/(i/len(test_images)))/60,2)

    print("Current Progress: ",np.round(i/len(test_images) * 100,2),"%")
    print("Current Run Time: ",np.round((stop-start) / 60,2),"minutes")
    print("Expected Run Time: ",expected_time,"minutes")
test_data = np.array(test_data)

print("Shape of new dataframes - {} , {}".format(test_data.shape, test_images.shape))

test_data=test_data*255

test_images

encoder = LabelEncoder()
encoder_df = pd.DataFrame(encoder.fit_transform(test_images[['sentiment']]))
test_images = test_images.join(encoder_df)
test_images.drop('sentiment',axis = 1,inplace=True)

test_label = test_images.iloc[:,1:]

test_label

test_label_categorical = tf.keras.utils.to_categorical(test_label,num_classes=2)
test_label_categorical

hist = model.fit(test_data,test_label_categorical, batch_size=5, epochs=64, validation_split=0.25)

model.evaluate(training_data,training_label_categorical)

model.evaluate(test_data,test_label_categorical)

def img_data(d): 
  tx = []
  img = url_to_image(d)
  image = cv2.resize(img,(224,224) ,interpolation = cv2.INTER_AREA)
  tx.append(image)
  tx = np.array(tx)
  return tx

def sentiment(p):
  if(p == 0):
    return 'Negative'
  else:
    return "Positive"

def get_prediction(x):
  tx = img_data(x)
  res = np.argmax(model.predict(tx))
  return sentiment(res)

l = 'https://th.bing.com/th/id/OIP.b-YLmeGfdPFHAAO-NyD2owHaHa?w=156&h=180&c=7&r=0&o=5&pid=1.7'
print("Predicted: ", get_prediction(l))

k = 'https://th.bing.com/th/id/OIP.AC14ntaV9uLkfI_I8Wx4PAHaEs?w=289&h=184&c=7&r=0&o=5&pid=1.7'
print("Predicted: ", get_prediction(k))

i = 'https://th.bing.com/th/id/OIP.G7i9YVdJ8A5bRqoOdPQVfAAAAA?w=116&h=180&c=7&r=0&o=5&pid=1.7'
print("predicted: ",get_prediction(i))

p = 'https://th.bing.com/th/id/OIP.r0aOnlQ51qfje_-pilV4hAHaJI?w=131&h=180&c=7&r=0&o=5&pid=1.7'
print("predicted: ",get_prediction(p))

u = 'https://th.bing.com/th/id/OIP.LLorE-TsPyNY_EAqNIDZYwHaHH?w=167&h=180&c=7&r=0&o=5&pid=1.7'
print("Predicted: ",get_prediction(u))

from PIL import Image
hun = 'https://th.bing.com/th/id/OIP.R78esYziCs7kUbBRP6Io8wAAAA?w=183&h=186&c=7&r=0&o=5&pid=1.7'
print("Predicted: ",get_prediction(hun))

model.save('best_model.h5')

